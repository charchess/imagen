# Imagen - Documentation Projet

> Documentation pour Claude Code - Vue d'ensemble compl√®te du projet
> Derni√®re mise √† jour : 2026-01-30

## üì∏ Vue d'Ensemble

**Imagen** est un service d'API de g√©n√©ration d'images IA bas√© sur **Stable Diffusion XL (SDXL)**. Il permet de g√©n√©rer des images √† partir de prompts textuels avec support du transfert de style via IP-Adapter.

**Optimis√© pour** : GPU RTX 2080 Ti (11GB VRAM)

---

## üèóÔ∏è Architecture

Architecture distribu√©e asynchrone avec 3 composants principaux :

### 1. API REST (api.py)
- **Framework** : FastAPI
- **Port** : 8009 (externe) ‚Üí 8000 (interne)
- **Endpoints** :
  - `POST /generate` - Soumettre une g√©n√©ration d'image, retourne job_id
  - `GET /status/{job_id}` - V√©rifier le statut (PENDING, PROGRESS, SUCCESS, FAILURE)
  - `GET /download/{filename}` - T√©l√©charger l'image PNG g√©n√©r√©e
  - `GET /health` - Health check
- **Limite** : Max 100 jobs en attente

### 2. Worker Celery (worker.py)
- **R√¥le** : Traitement GPU intensif des g√©n√©rations d'images
- **Configuration critique** :
  - Concurrence = 1 worker (essentiel pour m√©moire GPU limit√©e)
  - Timeout = 10 minutes par t√¢che
  - Retry = 3 tentatives avec backoff 10s sur OOM
  - Prefetch = 1 (une t√¢che √† la fois)
- **Fonctionnement** :
  1. Re√ßoit la requ√™te (prompt, negative_prompt, ip_strength)
  2. G√©n√®re un ID unique (UUID + timestamp)
  3. Charge l'image de r√©f√©rence si disponible
  4. G√©n√®re l'image via pipeline SDXL
  5. Sauvegarde en PNG dans `/outputs/`
  6. Nettoie la m√©moire GPU

### 3. Pipeline de G√©n√©ration (pipeline.py)
- **Classe** : `ElectraPipeline` (Singleton, lazy-loading)
- **Optimisations m√©moire** :
  - `enable_model_cpu_offload()` - Charge/d√©charge les couches √† la demande
  - `enable_vae_slicing()` - Traite l'image en tranches
  - `enable_vae_tiling()` - Traitement en tuiles pour √©conomiser VRAM
- **Features** :
  - Support prompts longs (via Compel avec dual text encoders)
  - IP-Adapter pour transfert de style depuis image de r√©f√©rence
  - Nettoyage automatique (garbage collection + CUDA cache clear)

---

## üõ†Ô∏è Stack Technique

| Composant | Technologie | Version | Usage |
|-----------|-------------|---------|-------|
| Web Framework | FastAPI | 0.105.0 | API REST |
| Task Queue | Celery | 5.3.4 | Distribution des t√¢ches |
| Message Broker | Redis | 5.0.1 | Backend de queue |
| ML Framework | PyTorch | 2.1.2 | Deep learning |
| Image Generation | Diffusers | 0.25.0 | Pipeline SDXL |
| Transformers | Transformers | 4.36.0 | Text encoding |
| Prompt Enhancement | Compel | 2.0.2 | Support prompts longs |
| IP-Adapter | h94/IP-Adapter | - | Transfert de style |
| Image Processing | Pillow | 10.1.0 | I/O images |
| Server | Uvicorn | 0.24.0 | ASGI server |
| Container | Docker + CUDA | 12.1 | D√©ploiement GPU |

---

## üìÅ Structure du Projet

```
imagen/
‚îú‚îÄ‚îÄ app/                          # Code principal
‚îÇ   ‚îú‚îÄ‚îÄ api.py                   # FastAPI REST API
‚îÇ   ‚îú‚îÄ‚îÄ worker.py                # Celery worker (t√¢ches GPU)
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py              # Pipeline SDXL
‚îÇ   ‚îî‚îÄ‚îÄ config.py                # Configuration & constantes
‚îú‚îÄ‚îÄ models/                       # Cache des mod√®les pr√©-entra√Æn√©s
‚îú‚îÄ‚îÄ outputs/                      # Images g√©n√©r√©es
‚îú‚îÄ‚îÄ reference/                    # Images de r√©f√©rence pour IP-Adapter
‚îÇ   ‚îî‚îÄ‚îÄ leona.jpg               # Image de test (200KB)
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestration (Redis, API, Worker)
‚îú‚îÄ‚îÄ dockerfile                   # Image container CUDA
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îú‚îÄ‚îÄ download_models.py           # Utilitaire t√©l√©chargement mod√®les
‚îú‚îÄ‚îÄ Makefile                     # Commandes d√©veloppement
‚îú‚îÄ‚îÄ .pre-commit-config.yaml      # Hooks qualit√© (ruff, mypy)
‚îî‚îÄ‚îÄ CLAUDE.md                    # Ce fichier
```

---

## üîÑ Flux de Donn√©es

```
Requ√™te Utilisateur
    ‚Üì
FastAPI POST /generate
    ‚Üì
Validation queue & cr√©ation t√¢che
    ‚Üì
Soumission Celery ‚Üí Redis
    ‚Üì
Worker Celery r√©cup√®re la t√¢che
    ‚Üì
Chargement pipeline SDXL (singleton, lazy-loaded)
    ‚Üì
Encodage prompts avec Compel (dual text encoders)
    ‚Üì
Chargement image r√©f√©rence + IP-Adapter (optionnel)
    ‚Üì
Inf√©rence SDXL (30 steps, CUDA GPU)
    ‚Üì
Nettoyage m√©moire (GC + CUDA cache)
    ‚Üì
Sauvegarde PNG ‚Üí outputs/
    ‚Üì
R√©sultat ‚Üí Redis
    ‚Üì
Utilisateur poll GET /status/{job_id}
    ‚Üì
T√©l√©chargement GET /download/{filename}
```

---

## ‚öôÔ∏è Configuration (config.py)

### Mod√®les
- **SDXL Base** : `stabilityai/stable-diffusion-xl-base-1.0`
- **VAE** : `madebyollin/sdxl-vae-fp16-fix` (correction artefacts noirs)
- **IP-Adapter** : `h94/IP-Adapter`

### Param√®tres de G√©n√©ration
- **Steps** : 30 (it√©rations d'inf√©rence)
- **Guidance Scale** : 7.5 (adh√©rence au prompt)
- **R√©solution** : 1024x1024 (native SDXL)

### Redis
- **URL** : `redis://localhost:6379/0`
- **Expiration r√©sultats** : 1 heure

---

## üöÄ Commandes Makefile

```bash
make setup      # Installer d√©pendances + cr√©er r√©pertoires
make dev        # Lancer FastAPI avec reload
make worker     # Lancer worker Celery localement
make flower     # Dashboard monitoring Celery (port 5555)
make up         # Docker Compose startup production
make down       # Docker Compose shutdown
make clean      # Nettoyer cache files
make gpu        # V√©rifier statut GPU (nvidia-smi)
make test       # Lancer pytest avec coverage
```

---

## üê≥ Docker Compose

### Services
1. **redis** : Message broker (port 6379)
2. **api** : FastAPI application (port 8009:8000)
3. **worker** : Celery worker (GPU-enabled)

Tous les services utilisent NVIDIA runtime pour acc√®s GPU.

### Volumes Mont√©s
- `./models:/app/models` - Cache mod√®les
- `./outputs:/app/outputs` - Images g√©n√©r√©es
- `./reference:/app/reference` - Images de r√©f√©rence

---

## üéØ Features Cl√©s

- ‚úÖ **Traitement Asynchrone** - G√©n√©ration non-bloquante via Celery
- ‚úÖ **Optimisation GPU** - Efficient en VRAM (offloading, slicing, tiling)
- ‚úÖ **Support Prompts Longs** - Via Compel (bypass limite 77 tokens)
- ‚úÖ **Transfert de Style** - IP-Adapter pour g√©n√©ration bas√©e sur image r√©f√©rence
- ‚úÖ **Tracking Statut** - Mises √† jour en temps r√©el
- ‚úÖ **Gestion Queue** - Pr√©vention surcharge serveur (max 100 jobs)
- ‚úÖ **Tol√©rance aux Pannes** - Retry automatique sur OOM
- ‚úÖ **Docker-Ready** - Containerisation compl√®te avec support GPU NVIDIA
- ‚úÖ **Outils Dev** - Pre-commit hooks, Makefile, type checking

---

## üîß Qualit√© du Code

### Pre-commit Hooks (.pre-commit-config.yaml)
- **Ruff** : Linter & formatter Python (auto-fix)
- **MyPy** : Type checking
- **Hooks standards** : Whitespace, file endings, YAML/JSON/TOML validation
- **Large file check** : Max 10MB (fichiers mod√®les)

### Tests
```bash
make test  # Pytest avec coverage
```

---

## üíæ Gestion M√©moire GPU

### Variables d'Environnement
```bash
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### Strat√©gies
1. **CPU Offload** - D√©chargement dynamique des couches
2. **VAE Slicing** - Traitement par tranches
3. **VAE Tiling** - Traitement par tuiles
4. **Garbage Collection** - Nettoyage apr√®s chaque g√©n√©ration
5. **CUDA Cache Clear** - Lib√©ration cache GPU
6. **Single Worker** - Une seule t√¢che √† la fois

---

## üìù Notes de D√©veloppement

### worker.py (Fichier Important)
C'est le c≈ìur du traitement GPU. Il :
- Configure Celery avec Redis
- D√©finit `generate_image_task()` comme t√¢che Celery bound
- G√®re la concurrence (CRITIQUE : 1 worker uniquement)
- Impl√©mente retry logic et timeout
- Nettoie la m√©moire GPU apr√®s chaque g√©n√©ration

### Debugging
```bash
# V√©rifier GPU
make gpu

# Monitoring Celery
make flower  # http://localhost:5555

# Logs
docker-compose logs -f worker
docker-compose logs -f api
```

---

## üé® Exemple d'Utilisation

```bash
# 1. D√©marrer les services
make up

# 2. Soumettre une g√©n√©ration
curl -X POST http://localhost:8009/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "a beautiful sunset over mountains",
    "negative_prompt": "blurry, low quality",
    "ip_strength": 0.0
  }'
# R√©ponse: {"job_id": "abc-123"}

# 3. V√©rifier le statut
curl http://localhost:8009/status/abc-123
# R√©ponse: {"state": "SUCCESS", "result": {...}}

# 4. T√©l√©charger l'image
curl http://localhost:8009/download/20260130_123456_abc123.png --output image.png
```

---

## üö® Points d'Attention

1. **Concurrence Worker = 1** : Ne JAMAIS augmenter (OOM GPU garanti)
2. **Queue Max = 100** : Protection contre surcharge
3. **Timeout = 10min** : G√©n√©rations complexes peuvent √™tre longues
4. **VAE Fix** : Utilisation obligatoire du VAE fix pour √©viter artefacts
5. **Compel** : N√©cessaire pour prompts > 77 tokens

---

## üìö Documentation du Projet

### Guides Utilisateur

- **[API.md](API.md)** - üìò R√©f√©rence API compl√®te (endpoints, sch√©mas, exemples)
- **[WORKFLOW.md](WORKFLOW.md)** - üîÑ Architecture technique d√©taill√©e (workflow, mod√®les, optimisations)
- **[API_UPDATES.md](API_UPDATES.md)** - üöÄ Nouvelles fonctionnalit√©s et mises √† jour

### Guides D√©veloppeur

- **[PONY_XL_INTEGRATION.md](PONY_XL_INTEGRATION.md)** - ü¶Ñ Guide d'impl√©mentation PonyXL v6 + LoRA/LyCORIS
- **[CLAUDE.md](CLAUDE.md)** - üìñ Ce fichier - Documentation projet compl√®te

## üîó R√©f√©rences Externes

- [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
- [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)
- [Compel](https://github.com/damian0815/compel)
- [Diffusers](https://huggingface.co/docs/diffusers)

---

**√âtat du Projet** : Production-ready ‚úÖ

---

## ‚ö†Ô∏è DevContainer - Acc√®s aux Images G√©n√©r√©es

### Probl√®me Technique

Dans un DevContainer avec Docker-from-Docker sur WSL2, le r√©pertoire `./outputs` local est DIFF√âRENT du volume Docker `/workspaces/imagen/outputs` utilis√© par les conteneurs. C'est d√ª √† l'isolation des namespaces.

**Les images SONT correctement sauvegard√©es** dans le volume Docker partag√© entre API et Worker, mais invisibles depuis le DevContainer local.

### Solutions

#### 1. Acc√®s via Docker (Recommand√©)
```bash
# Lister les images
docker run --rm -v /workspaces/imagen/outputs:/outputs alpine ls -lh /outputs/

# Copier UNE image vers local
docker run --rm -v /workspaces/imagen/outputs:/outputs alpine cat /outputs/IMAGE.png > ./IMAGE.png

# Copier TOUTES les images
docker run --rm -v /workspaces/imagen/outputs:/src -v ${PWD}:/dest alpine sh -c "cp /src/*.png /dest/"
```

#### 2. Acc√®s via API (Cas d'usage normal)
Les images sont accessibles via l'API sur `/outputs/<filename>` gr√¢ce au `StaticFiles` mount.

```bash
# Dans un navigateur ou depuis l'h√¥te
curl http://localhost:8009/outputs/20260130_043240_aaad6da8.png --output image.png
```

#### 3. Utiliser le script helper
```bash
./show-outputs.sh list   # Liste les images
./show-outputs.sh sync   # Copie toutes vers outputs_local/
```

### V√©rification que le Syst√®me Fonctionne

```bash
# Les conteneurs voient bien les images :
docker run --rm -v /workspaces/imagen/outputs:/test alpine ls -lh /test/
# ‚úÖ Doit afficher toutes les images

# Le DevContainer NE voit PAS les images (c'est normal) :
ls /workspaces/imagen/outputs/
# Peut √™tre vide ou obsol√®te
```

**Conclusion** : Le syst√®me fonctionne parfaitement. C'est juste l'acc√®s depuis le DevContainer qui n√©cessite docker run.
